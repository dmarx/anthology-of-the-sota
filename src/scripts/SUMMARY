================================================================================
File: src/scripts/generate-package-lock.js
================================================================================
// src/scripts/generate-package-lock.js
const fs = require('fs');
const { execSync } = require('child_process');
const path = require('path');

function generatePackageLock() {
  const frontendDir = path.join(__dirname, '../../frontend');
  
  // Check if we need to generate package-lock.json
  if (!fs.existsSync(path.join(frontendDir, 'package-lock.json'))) {
    console.log('Generating package-lock.json...');
    
    try {
      // Run npm install to generate package-lock.json
      execSync('npm install', { 
        cwd: frontendDir,
        stdio: 'inherit'
      });
      
      console.log('Successfully generated package-lock.json');
    } catch (error) {
      console.error('Error generating package-lock.json:', error);
      process.exit(1);
    }
  } else {
    console.log('package-lock.json already exists');
  }
}

// Run if called directly
if (require.main === module) {
  generatePackageLock();
}

module.exports = generatePackageLock;



================================================================================
File: src/scripts/generate_summaries/__init__.py
================================================================================
"""Package for generating directory summaries to assist LLM interactions."""
from pathlib import Path
from typing import List

__version__ = "0.1.0"

# Re-export main functionality
from .generator import SummaryGenerator

__all__ = ["SummaryGenerator"]



================================================================================
File: src/scripts/generate_summaries/__main__.py
================================================================================
import subprocess
from pathlib import Path
from typing import Optional

def commit_and_push(
    message: str,
    branch: str,
    paths: list[str | Path],
    base_branch: Optional[str] = None,
    force: bool = False
) -> None:
    """Commit changes and push to specified branch.
    
    Args:
        message: Commit message
        branch: Branch to push to
        paths: List of paths to commit
        base_branch: Optional base branch to create new branch from
        force: If True, create fresh branch and force push (for generated content)
    """
    # Convert paths to strings
    path_strs = [str(p) for p in paths]
    
    # Set up git config
    subprocess.run(["git", "config", "--local", "user.email", "github-actions[bot]@users.noreply.github.com"])
    subprocess.run(["git", "config", "--local", "user.name", "github-actions[bot]"])
    
    if force:
        # Create fresh branch from base_branch or HEAD
        base = base_branch or "HEAD"
        logger.info(f"Creating fresh branch {branch} from {base}")
        subprocess.run(["git", "checkout", "-B", branch, base])
    else:
        # Normal branch handling
        if base_branch:
            logger.info(f"Creating new branch {branch} from {base_branch}")
            subprocess.run(["git", "checkout", "-b", branch, base_branch])
        else:
            logger.info(f"Switching to branch {branch}")
            subprocess.run(["git", "checkout", "-b", branch])
            subprocess.run(["git", "pull", "origin", branch], capture_output=True)
    
    # Stage and commit changes
    subprocess.run(["git", "add", *path_strs])
    
    # Only commit if there are changes
    result = subprocess.run(
        ["git", "diff", "--staged", "--quiet"],
        capture_output=True
    )
    if result.returncode == 1:  # Changes exist
        logger.info("Committing changes")
        subprocess.run(["git", "commit", "-m", message])
        
        # Push changes
        if force:
            logger.info(f"Force pushing {branch} branch")
            subprocess.run(["git", "push", "-f", "origin", branch])
        else:
            logger.info("Pushing changes")
            subprocess.run(["git", "push", "origin", branch])
    else:
        logger.info("No changes to commit")


"""CLI entry point for summary generator."""
import fire
from loguru import logger
from pathlib import Path
from . import generator
#from readme_generator.utils import commit_and_push
from . import special_summaries


def generate(root_dir: str = ".", push: bool = True) -> list[Path]:
    """Generate directory summaries and special summaries.
    
    Args:
        root_dir: Root directory to generate summaries for
        push: Whether to commit and push changes
        
    Returns:
        List of paths to generated summary files
    """
    logger.info(f"Generating summaries for {root_dir}")
    
    # Generate regular directory summaries
    gen = generator.SummaryGenerator(root_dir)
    summary_files = gen.generate_all_summaries()
    
    # Generate special summaries
    special_files = special_summaries.generate_special_summaries(root_dir)
    all_files = summary_files + special_files
    
    if push:
        logger.info("Committing and pushing changes")
        commit_and_push(
            message="Update directory summaries and special summaries",
            branch="summaries",
            paths=all_files,
            base_branch="main",
            force=True  # Use force push for generated content
        )
    
    return all_files

def main():
    """CLI entry point."""
    fire.Fire(generate)

if __name__ == "__main__":
    main()



================================================================================
File: src/scripts/generate_summaries/generator.py
================================================================================
"""Core summary generation functionality."""
from pathlib import Path
from typing import List, Set
from loguru import logger

class SummaryGenerator:
    """Generate summary files for each directory in the project."""
    
    def __init__(self, root_dir: str | Path):
        """Initialize generator with root directory.
        
        Args:
            root_dir: Root directory to generate summaries for
        """
        self.root_dir = Path(root_dir)
        
    def should_include_file(self, file_path: Path) -> bool:
        """Determine if a file should be included in the summary.
        
        Args:
            file_path: Path to file to check
            
        Returns:
            True if file should be included in summary
        """
        # Skip common files we don't want to summarize
        excluded_files = {
            '.git', '.gitignore', '.pytest_cache', '__pycache__',
            'SUMMARY', '.coverage', '.env', '.venv', '.idea', '.vscode'
        }
        
        # Skip excluded directories and files
        if any(part in excluded_files for part in file_path.parts):
            return False
            
        # Skip .github/workflows directory
        if '.github/workflows' in str(file_path):
            return False
            
        # Only include text files
        text_extensions = {'.py', '.md', '.txt', '.yml', '.yaml', '.toml', 
                         '.json', '.html', '.css', '.js', '.j2'}
        return file_path.suffix in text_extensions
    
    def should_include_directory(self, directory: Path) -> bool:
        """Determine if a directory should have a summary generated.
        
        Args:
            directory: Directory to check
            
        Returns:
            True if directory should have a summary
        """
        # Skip .github/workflows directory
        if '.github/workflows' in str(directory):
            return False
            
        # Skip other excluded directories
        excluded_dirs = {
            '.git', '__pycache__', '.pytest_cache',
            '.venv', '.idea', '.vscode'
        }
        
        return not any(part in excluded_dirs for part in directory.parts)
    
    def _collect_directories(self) -> Set[Path]:
        """Collect all directories containing files to summarize.
        
        Returns:
            Set of directory paths
        """
        directories = set()
        for file_path in self.root_dir.rglob('*'):
            if (file_path.is_file() and 
                self.should_include_file(file_path) and
                self.should_include_directory(file_path.parent)):
                directories.add(file_path.parent)
        return directories
        
    def generate_directory_summary(self, directory: Path) -> str:
        """Generate a summary for a single directory.
        
        Args:
            directory: Directory to generate summary for
            
        Returns:
            Generated summary text
        """
        logger.debug(f"Generating summary for {directory}")
        summary = []
        
        # Process all files in the directory
        for file_path in sorted(directory.rglob('*')):
            if not file_path.is_file() or not self.should_include_file(file_path):
                continue
                
            try:
                # Get relative path from root for the header
                rel_path = file_path.relative_to(self.root_dir)
                
                # Read file content
                content = file_path.read_text(encoding='utf-8')
                
                # Add to summary with clear separation
                summary.extend([
                    '=' * 80,
                    f'File: {rel_path}',
                    '=' * 80,
                    content,
                    '\n'  # Extra newline for separation
                ])
            except Exception as e:
                logger.error(f"Error processing {file_path}: {e}")
                
        return '\n'.join(summary)
        
    def generate_all_summaries(self) -> List[Path]:
        """Generate summary files for all directories.
        
        Returns:
            List of paths to generated summary files
        """
        logger.info("Starting summary generation")
        summary_files = []
        
        # Collect directories
        directories = self._collect_directories()
        logger.info(f"Found {len(directories)} directories to process")
        
        # Generate summaries
        for directory in sorted(directories):
            if not self.should_include_directory(directory):
                continue
                
            summary_content = self.generate_directory_summary(directory)
            summary_path = directory / 'SUMMARY'
            
            try:
                summary_path.write_text(summary_content, encoding='utf-8')
                logger.info(f"Generated summary for {directory}")
                summary_files.append(summary_path)
            except Exception as e:
                logger.error(f"Error writing summary for {directory}: {e}")
                
        return summary_files



================================================================================
File: src/scripts/generate_summaries/signature_extractor.py
================================================================================
# signature_extractor.py
"""Extracts and formats Python code signatures with proper nesting."""
import ast
from dataclasses import dataclass
from pathlib import Path
from typing import List, Dict
from loguru import logger

@dataclass
class Signature:
    """Represents a Python function or class signature with documentation."""
    name: str
    kind: str  # 'function', 'method', or 'class'
    args: list[str]
    returns: str | None
    docstring: str | None
    decorators: list[str]
    methods: list['Signature']  # For storing class methods

class ParentNodeTransformer(ast.NodeTransformer):
    """Add parent references to all nodes in the AST."""
    
    def visit(self, node: ast.AST) -> ast.AST:
        """Visit a node and add parent references to all its children."""
        for child in ast.iter_child_nodes(node):
            child.parent = node
        return super().visit(node)

class SignatureExtractor:
    """Extracts detailed signatures from Python files."""
    
    def get_type_annotation(self, node: ast.AST) -> str:
        """Convert AST annotation node to string representation."""
        if isinstance(node, ast.Name):
            return node.id
        elif isinstance(node, ast.Constant):
            return repr(node.value)
        elif isinstance(node, ast.Subscript):
            container = self.get_type_annotation(node.value)
            params = self.get_type_annotation(node.slice)
            return f"{container}[{params}]"
        elif isinstance(node, ast.BinOp):
            left = self.get_type_annotation(node.left)
            right = self.get_type_annotation(node.right)
            return f"{left} | {right}"
        elif isinstance(node, ast.Tuple):
            elts = [self.get_type_annotation(e) for e in node.elts]
            return f"[{', '.join(elts)}]"
        return "Any"
    
    def get_arg_string(self, arg: ast.arg) -> str:
        """Convert function argument to string with type annotation."""
        arg_str = arg.arg
        if arg.annotation:
            type_str = self.get_type_annotation(arg.annotation)
            arg_str += f": {type_str}"
        return arg_str

    def extract_signatures(self, source: str) -> List[Signature]:
        """Extract all function and class signatures from source code."""
        try:
            # Parse and add parent references
            tree = ast.parse(source)
            transformer = ParentNodeTransformer()
            transformer.visit(tree)
            
            signatures: List[Signature] = []
            classes: Dict[ast.ClassDef, Signature] = {}
            
            for node in ast.walk(tree):
                # Handle functions
                if isinstance(node, (ast.FunctionDef, ast.AsyncFunctionDef)):
                    args = []
                    for arg in node.args.args:
                        args.append(self.get_arg_string(arg))
                    
                    returns = None
                    if node.returns:
                        returns = self.get_type_annotation(node.returns)
                    
                    decorators = []
                    for decorator in node.decorator_list:
                        if isinstance(decorator, ast.Name):
                            decorators.append(f"@{decorator.id}")
                        elif isinstance(decorator, ast.Call):
                            if isinstance(decorator.func, ast.Name):
                                decorators.append(f"@{decorator.func.id}(...)")
                    
                    sig = Signature(
                        name=node.name,
                        kind='method' if hasattr(node, 'parent') and isinstance(node.parent, ast.ClassDef) else 'function',
                        args=args,
                        returns=returns,
                        docstring=ast.get_docstring(node),
                        decorators=decorators,
                        methods=[]
                    )
                    
                    # Add to appropriate parent
                    if hasattr(node, 'parent') and isinstance(node.parent, ast.ClassDef) and node.parent in classes:
                        classes[node.parent].methods.append(sig)
                    else:
                        signatures.append(sig)
                
                # Handle classes
                elif isinstance(node, ast.ClassDef):
                    bases = []
                    for base in node.bases:
                        if isinstance(base, ast.Name):
                            bases.append(base.id)
                    
                    decorators = []
                    for decorator in node.decorator_list:
                        if isinstance(decorator, ast.Name):
                            decorators.append(f"@{decorator.id}")
                    
                    class_sig = Signature(
                        name=node.name,
                        kind='class',
                        args=bases,
                        returns=None,
                        docstring=ast.get_docstring(node),
                        decorators=decorators,
                        methods=[]
                    )
                    
                    classes[node] = class_sig
                    signatures.append(class_sig)
                    
            return signatures
        except Exception as e:
            logger.error(f"Error parsing source: {e}")
            return []

    def format_signature(self, sig: Signature, indent: int = 0) -> List[str]:
        """Format a signature for display with proper indentation."""
        lines = []
        indent_str = "    " * indent
        
        # Add decorators
        for decorator in sig.decorators:
            lines.append(f"{indent_str}{decorator}")
        
        # Format the signature line
        if sig.kind == 'class':
            base_str = f"({', '.join(sig.args)})" if sig.args else ""
            lines.append(f"{indent_str}class {sig.name}{base_str}")
        else:
            async_prefix = "async " if "async" in sig.decorators else ""
            args_str = ", ".join(sig.args)
            return_str = f" -> {sig.returns}" if sig.returns else ""
            lines.append(f"{indent_str}{async_prefix}def {sig.name}({args_str}){return_str}")
        
        # Add docstring if present
        if sig.docstring:
            doc_lines = sig.docstring.split('\n')
            if len(doc_lines) == 1:
                lines.append(f'{indent_str}    """{sig.docstring}"""')
            else:
                lines.append(f'{indent_str}    """')
                for doc_line in doc_lines:
                    if doc_line.strip():
                        lines.append(f"{indent_str}    {doc_line}")
                lines.append(f'{indent_str}    """')
        
        # Add methods for classes
        if sig.methods:
            lines.append("")  # Add spacing
            for method in sig.methods:
                lines.extend(self.format_signature(method, indent + 1))
                lines.append("")  # Add spacing between methods
        
        return lines

def generate_python_summary(root_dir: str | Path) -> str:
    """Generate enhanced Python project structure summary.
    
    Args:
        root_dir: Root directory of the project
        
    Returns:
        Formatted markdown string of Python signatures
    """
    root_dir = Path(root_dir)
    extractor = SignatureExtractor()
    content = ["# Python Project Structure\n"]
    
    for file in sorted(root_dir.rglob("*.py")):
        if any(part.startswith('.') for part in file.parts):
            continue
        if '__pycache__' in file.parts:
            continue
            
        try:
            # Get relative path
            rel_path = file.relative_to(root_dir)
            
            # Read and extract signatures
            source = file.read_text()
            signatures = extractor.extract_signatures(source)
            
            # Only include files that have actual content
            if signatures:
                content.append(f"## {rel_path}")
                content.append("```python")
                
                # Format each signature
                for sig in signatures:
                    content.extend(extractor.format_signature(sig))
                    content.append("")  # Add spacing between top-level items
                
                content.append("```\n")
            
        except Exception as e:
            logger.error(f"Error processing {file}: {e}")
    
    return "\n".join(content)



================================================================================
File: src/scripts/generate_summaries/special_summaries.py
================================================================================
"""Special summary generators for project-wide summaries."""
from pathlib import Path
from typing import List
from loguru import logger
from .signature_extractor import SignatureExtractor, generate_python_summary  # New import

class SpecialSummariesGenerator:
    """Generate special project-wide summary files."""
    
    def __init__(self, root_dir: str | Path):
        """Initialize generator with root directory."""
        self.root_dir = Path(root_dir)
        self.summaries_dir = self.root_dir / "SUMMARIES"
        self.signature_extractor = SignatureExtractor()  # New instance
    
    def _find_readmes(self, include_root: bool = True) -> List[Path]:
        """Find all README files in the project."""
        readmes = []
        for file in self.root_dir.rglob("README.md"):
            if not include_root and file.parent == self.root_dir:
                continue
            readmes.append(file)
        return sorted(readmes)
    
    def generate_special_summaries(self) -> List[Path]:
        """Generate all special summary files.
        
        Returns:
            List of paths to generated summary files
        """
        self.summaries_dir.mkdir(exist_ok=True)
        generated_files = []
        
        # Generate READMEs.md
        readmes_path = self.summaries_dir / "READMEs.md"
        readme_content = []
        for readme in self._find_readmes(include_root=True):
            rel_path = readme.relative_to(self.root_dir)
            readme_content.extend([
                "=" * 80,
                f"# {rel_path}",
                "=" * 80,
                readme.read_text(),
                "\n"
            ])
        readmes_path.write_text("\n".join(readme_content))
        generated_files.append(readmes_path)
        
        # Generate README_SUBs.md
        subs_path = self.summaries_dir / "README_SUBs.md"
        subs_content = []
        for readme in self._find_readmes(include_root=False):
            rel_path = readme.relative_to(self.root_dir)
            subs_content.extend([
                "=" * 80,
                f"# {rel_path}",
                "=" * 80,
                readme.read_text(),
                "\n"
            ])
        subs_path.write_text("\n".join(subs_content))
        generated_files.append(subs_path)
        
        # Generate enhanced PYTHON.md
        python_path = self.summaries_dir / "PYTHON.md"
        python_content = generate_python_summary(self.root_dir)  # Using new generator
        python_path.write_text(python_content)
        generated_files.append(python_path)
        
        return generated_files

def generate_special_summaries(root_dir: str | Path = ".") -> List[Path]:
    """Generate special summaries for the project."""
    generator = SpecialSummariesGenerator(root_dir)
    return generator.generate_special_summaries()



================================================================================
File: src/scripts/readme_generator.py
================================================================================
from pathlib import Path
from typing import List
from loguru import logger
from jinja2 import Environment, FileSystemLoader
from .utils import load_config, get_project_root, commit_and_push

def get_section_templates(template_dir: Path) -> List[str]:
    """Get all section templates in proper order.
    
    Args:
        template_dir: Path to template directory containing sections/
        
    Returns:
        List of template names in desired order
    """
    # Define section order
    section_order = {
        "introduction.md.j2": 0,
        "prerequisites.md.j2": 1,
        "usage.md.j2": 2,
        "development.md.j2": 3,
        "summaries.md.j2": 4,
        "site.md.j2": 5,
        "structure.md.j2": 6,
        "todo.md.j2": 999  # Always last if present
    }
    
    sections_dir = template_dir / "sections"
    templates = []
    
    # Collect all template files
    for file in sections_dir.glob("*.md.j2"):
        # Skip todo if it doesn't exist (it's optional)
        if file.name == "todo.md.j2" and not file.exists():
            continue
        templates.append(file.name)
    
    # Sort by explicit order, then alphabetically for any new sections
    return sorted(
        templates,
        key=lambda x: section_order.get(x, 500)
    )

def generate_readme() -> None:
    """Generate README from templates and commit changes"""
    project_root = get_project_root()
    logger.debug(f"Project root identified as: {project_root}")
    
    logger.info("Loading configurations")
    project_config = load_config("pyproject.toml")
    
    logger.info("Setting up Jinja2 environment")
    template_dir = project_root / 'docs/readme'
    logger.debug(f"Template directory: {template_dir}")
    
    env = Environment(
        loader=FileSystemLoader(template_dir),
        trim_blocks=True,
        lstrip_blocks=True
    )
    
    # Add template utility functions
    env.globals['get_section_templates'] = lambda: get_section_templates(template_dir)
    
    template = env.get_template('base.md.j2')
    
    variables = {
        'project': project_config['project'],
        'readme': project_config['tool']['readme'],
    }
    
    logger.info("Rendering README template")
    output = template.render(**variables)
    
    readme_path = project_root / 'README.llm'
    logger.debug(f"Writing README to: {readme_path}")
    readme_path.write_text(output)
    
    logger.info("Committing changes")
    commit_and_push(readme_path)

if __name__ == "__main__":
    generate_readme()



================================================================================
File: src/scripts/registry/__init__.py
================================================================================
# scripts/registry/__init__.py
"""ML Training Recommendations Registry.

A system for tracking, managing, and organizing machine learning training recommendations
from research papers with unique identifiers and status tracking.
"""

from .types import MLRStatus, Recommendation, Source, Evidence
from .identifiers import MLRIdentifierRegistry
from .recommendations import RecommendationRegistry, build_registry_from_yaml
from .io import (
    load_research_yaml,
    save_registry,
    load_registry,
    registry_to_markdown,
    RegistryDataError
)

__version__ = "0.1.0"

__all__ = [
    'MLRStatus',
    'Recommendation',
    'Source',
    'Evidence',
    'MLRIdentifierRegistry',
    'RecommendationRegistry',
    'build_registry_from_yaml',
    'load_research_yaml',
    'save_registry',
    'load_registry',
    'registry_to_markdown',
    'RegistryDataError',
]



================================================================================
File: src/scripts/registry/cli.py
================================================================================
# src/scripts/registry/cli.py
"""Command-line interface for registry operations."""

import fire
from pathlib import Path
from loguru import logger
from typing import Optional
from . import (
    build_registry_from_yaml,
    load_research_yaml, 
    save_registry,
    registry_to_markdown
)
#from ..utils import commit_and_push
from llamero.utils import commit_and_push_to_branch #commit_and_push

def build(
    input_path: str | Path = "data/research.yaml",
    output_dir: str | Path = "data",
    push: bool = True,
    branch: Optional[str] = "HEAD"
) -> None:
    """Build registry from research YAML and generate outputs.
    
    Args:
        input_path: Path to research YAML file
        output_dir: Directory to save outputs (default: data directory)
        push: Whether to commit and push changes
        branch: Optional branch name to commit to (default: current branch)
    """
    logger.info(f"Building registry from {input_path}")
    output_dir = Path(output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # Generate registry
    yaml_data = load_research_yaml(input_path)
    registry = build_registry_from_yaml(yaml_data)
    
    # Save outputs
    registry_yaml = output_dir / "registry.yaml"
    registry_md = output_dir / "REGISTRY.md"
    
    save_registry(registry, registry_yaml)
    registry_to_markdown(registry, registry_md)
    logger.info(f"Registry outputs saved to {output_dir}")

    rdme= output_dir.parent / "docs/readme/sections/registry.md.j2"
    registry_to_markdown(registry, rdme)
    
    
    if push:
        logger.info("Committing changes")
        #commit_and_push([registry_yaml, registry_md, rdme])
        commit_and_push_to_branch(
            message="Update ML training registry",
            branch=branch or "main",
            paths=[registry_yaml, registry_md, rdme],
            force=False
        )


def cli():
    """CLI entry point."""
    return fire.Fire({
        'build': build
    })


if __name__ == "__main__":
    cli()



================================================================================
File: src/scripts/registry/identifiers.py
================================================================================
# scripts/registry/identifiers.py
"""MLR identifier generation and management."""

import json
import os
import re
from collections import defaultdict
from pathlib import Path
from typing import Dict, Optional
import logging

logger = logging.getLogger(__name__)

class MLRIdentifierRegistry:
    """Manages unique identifiers for ML recommendations."""
    
    def __init__(self, registry_file: str | Path = "mlr_registry.json"):
        """Initialize the identifier registry.
        
        Args:
            registry_file: Path to the JSON file storing ID mappings
        """
        self.registry_file = Path(registry_file)
        self.current_ids = self._load_registry()
        self.author_counters = self._initialize_author_counters()
        
    def _load_registry(self) -> Dict[str, Dict]:
        """Load existing MLR IDs from registry file."""
        try:
            if self.registry_file.exists():
                with open(self.registry_file, 'r') as f:
                    return json.load(f)
            logger.info(f"No existing registry found at {self.registry_file}")
        except json.JSONDecodeError:
            logger.warning(f"Error reading registry file {self.registry_file}. Starting fresh.")
        return {'paper_ids': {}, 'recommendation_ids': {}}
    
    def _initialize_author_counters(self) -> Dict[str, int]:
        """Initialize counters for author IDs from existing registry."""
        counters = defaultdict(int)
        for paper_id in self.current_ids['paper_ids'].values():
            if match := re.match(r'([A-Za-z]+)(\d+)', paper_id):
                author, num = match.groups()
                counters[author] = max(counters[author], int(num))
        return counters
    
    def _save_registry(self) -> None:
        """Save current MLR IDs to registry file."""
        self.registry_file.parent.mkdir(parents=True, exist_ok=True)
        with open(self.registry_file, 'w') as f:
            json.dump(self.current_ids, f, indent=2)
        logger.debug(f"Saved registry to {self.registry_file}")
    
    def get_paper_id(self, first_author: str, year: int, arxiv_id: Optional[str] = None) -> str:
        """Get or generate a paper identifier.
        
        Args:
            first_author: First author's name
            year: Publication year
            arxiv_id: Optional arXiv identifier
            
        Returns:
            Unique paper identifier string
        """
        paper_key = f"{first_author}-{year}-{arxiv_id if arxiv_id else 'none'}"
        
        if paper_key in self.current_ids['paper_ids']:
            return self.current_ids['paper_ids'][paper_key]
        
        author_base = re.sub(r'[^A-Za-z]', '', first_author)
        self.author_counters[author_base] += 1
        paper_id = f"{author_base}{self.author_counters[author_base]:03d}"
        
        self.current_ids['paper_ids'][paper_key] = paper_id
        self._save_registry()
        logger.debug(f"Generated new paper ID {paper_id} for {paper_key}")
        
        return paper_id
            
    def generate_id(self, year: int, paper_id: str) -> str:
        """Generate a new unique MLR identifier.
        
        Args:
            year: Publication year
            paper_id: Unique paper identifier
            
        Returns:
            MLR identifier string
        """
        key = f"{year}-{paper_id}"
        if key not in self.current_ids['recommendation_ids']:
            self.current_ids['recommendation_ids'][key] = 0
        
        self.current_ids['recommendation_ids'][key] += 1
        mlr_id = f"MLR-{year}-{paper_id}-{self.current_ids['recommendation_ids'][key]:04d}"
        self._save_registry()
        logger.debug(f"Generated new MLR ID {mlr_id}")
        return mlr_id



================================================================================
File: src/scripts/registry/io.py
================================================================================
# src/scripts/registry/io.py
"""I/O operations for the ML recommendation registry."""

import yaml
import json
from pathlib import Path
from typing import Dict, Union
from loguru import logger
from datetime import datetime
from collections import defaultdict

from .recommendations import RecommendationRegistry
from .types import MLRStatus

class RegistryDataError(Exception):
    """Raised when there are issues with registry data format."""
    pass

def validate_paper_entry(paper: Dict, year: str) -> None:
    """Validate a single paper entry.
    
    Args:
        paper: Dictionary containing paper data
        year: Year the paper is from (for error messages)
        
    Raises:
        RegistryDataError: If paper data is invalid
    """
    required_fields = {
        'title': str,
        'first_author': str,
        'year': int,
        #'topics': list
    }

    # Check for missing required fields
    missing_fields = [field for field in required_fields if field not in paper]
    if missing_fields:
        logger.warning(paper)
        raise RegistryDataError(
            f"Paper in year {year} missing required fields: {', '.join(missing_fields)}"
        )

    # Check field types
    # for field, expected_type in required_fields.items():
    #     if not isinstance(paper[field], expected_type):
    #         logger.warning(paper)
    #         raise RegistryDataError(
    #             f"Paper in year {year} has invalid type for {field}: "
    #             f"expected {expected_type.__name__}, got {type(paper[field]).__name__}"
    #         )

    # # Validate year matches container
    # if str(paper['year']) != str(year):
    #     raise RegistryDataError(
    #         f"Paper year {paper['year']} doesn't match container year {year}"
    #     )

    # # Validate topics not empty
    # if not paper['topics']:
    #     raise RegistryDataError(
    #         f"Paper '{paper['title']}' ({year}) has empty topics list"
    #     )

    # Validate topics are strings
    # if not all(isinstance(topic, str) for topic in paper['topics']):
    #     raise RegistryDataError(
    #         f"Paper '{paper['title']}' ({year}) has non-string topics"
    #     )

    # # If SOTA recommendations present, validate they're strings
    # if 'sota' in paper and not all(isinstance(rec, str) for rec in paper['sota']):
    #     raise RegistryDataError(
    #         f"Paper '{paper['title']}' ({year}) has non-string SOTA recommendations"
    #     )

def load_research_yaml(file_path: Union[str, Path]) -> Dict:
    """Load research data from YAML file.
    
    Args:
        file_path: Path to the YAML file
        
    Returns:
        Dictionary containing the research data
        
    Raises:
        FileNotFoundError: If the file doesn't exist
        yaml.YAMLError: If the file contains invalid YAML
        RegistryDataError: If the data format is invalid
    """
    file_path = Path(file_path)
    if not file_path.exists():
        raise FileNotFoundError(f"Research data file not found: {file_path}")

    try:
        with open(file_path, 'r') as f:
            data = yaml.safe_load(f)
    except yaml.YAMLError as e:
        logger.error(f"Error parsing YAML file {file_path}: {e}")
        raise

    # Validate basic data structure
    if not isinstance(data, dict):
        raise RegistryDataError("Research data must be a dictionary")

    # Validate years and entries
    for year, papers in data.items():
        try:
            year_int = int(year)
            if not 1900 <= year_int <= datetime.now().year:
                raise RegistryDataError(f"Invalid year: {year}")
        except ValueError:
            raise RegistryDataError(f"Invalid year format: {year}")

        if not isinstance(papers, list):
            raise RegistryDataError(f"Papers for year {year} must be a list")

        # Validate each paper
        for paper in papers:
            if not isinstance(paper, dict):
                raise RegistryDataError(f"Invalid paper entry in year {year}")
            validate_paper_entry(paper, year)

    return data

def save_registry(registry: RecommendationRegistry, output_file: Union[str, Path]) -> None:
    """Save registry to a file.
    
    Args:
        registry: RecommendationRegistry instance
        output_file: Path where to save the file
    """
    output_file = Path(output_file)
    output_file.parent.mkdir(parents=True, exist_ok=True)
    
    data = registry.export_registry()
    
    try:
        # Write as JSONL if .jsonl extension
        if output_file.suffix == '.jsonl':
            with open(output_file, 'w') as f:
                for rec in data['recommendations']:
                    f.write(json.dumps(rec) + '\n')
        # Write as YAML for other extensions
        else:
            with open(output_file, 'w') as f:
                yaml.safe_dump(
                    data,
                    f,
                    sort_keys=False,
                    allow_unicode=True,
                    default_flow_style=False
                )
        logger.info(f"Registry saved to {output_file}")
    except Exception as e:
        logger.error(f"Error saving registry to {output_file}: {e}")
        raise

def load_registry(file_path: Union[str, Path]) -> Dict:
    """Load a saved registry file.
    
    Args:
        file_path: Path to the registry file
        
    Returns:
        Dictionary containing the registry data
    """
    file_path = Path(file_path)
    if not file_path.exists():
        raise FileNotFoundError(f"Registry file not found: {file_path}")
    
    try:
        # Handle JSONL format
        if file_path.suffix == '.jsonl':
            recommendations = []
            with open(file_path, 'r') as f:
                for line in f:
                    if line.strip():
                        recommendations.append(json.loads(line))
            data = {
                'metadata': {
                    'schema_version': '1.0',
                    'last_updated': datetime.now().strftime('%Y-%m-%d')
                },
                'recommendations': recommendations
            }
        # Handle YAML format
        else:
            with open(file_path, 'r') as f:
                data = yaml.safe_load(f)
    except (yaml.YAMLError, json.JSONDecodeError) as e:
        logger.error(f"Error parsing registry file {file_path}: {e}")
        raise
    
    # Basic validation
    if not isinstance(data, dict):
        raise RegistryDataError("Invalid registry format")
    
    if 'recommendations' not in data:
        raise RegistryDataError("Registry missing recommendations")
    
    if not isinstance(data['recommendations'], list):
        raise RegistryDataError("Recommendations must be a list")
        
    return data

def registry_to_markdown(registry: RecommendationRegistry, output_file: Union[str, Path]) -> None:
    """Export registry to a markdown document."""
    output_file = Path(output_file)
    output_file.parent.mkdir(parents=True, exist_ok=True)
    
    data = registry.export_registry()
    
    with open(output_file, 'w') as f:
        f.write("# ML Training Recommendations Registry\n\n")
        f.write(f"Last updated: {data['metadata']['last_updated']}\n\n")
        
        # Group recommendations by status and topic for display
        status_topics = defaultdict(lambda: defaultdict(list))
        for rec in data['recommendations']:
            status = rec.get('status', 'standard')
            topic = rec.get('topic', 'uncategorized')
            status_topics[status][topic].append(rec)
        
        # Write recommendations by status and topic
        for status in MLRStatus:
            f.write(f"## {status.value.title()} Recommendations\n\n")
            
            for topic, recs in sorted(status_topics[status.value].items()):
                if recs:
                    f.write(f"### {topic}\n\n")
                    for rec in recs:
                        f.write(f"- {rec['recommendation']}\n")
                        if rec.get('implementations'):
                            f.write(f"  - Implementations: {', '.join(rec['implementations'])}\n")
                        f.write("\n")
            f.write("\n")
        
        # Write statistics
        f.write("## Statistics\n\n")
        for topic_data in data['topics'].items():
            topic, stats = topic_data
            f.write(f"### {topic}\n\n")
            f.write(f"- Total recommendations: {stats['count']}\n")
            if stats['years']['earliest'] and stats['years']['latest']:
                f.write(f"- Year range: {stats['years']['earliest']} - {stats['years']['latest']}\n")
            f.write("\n")



================================================================================
File: src/scripts/registry/recommendations.py
================================================================================
# src/scripts/registry/recommendations.py
"""Core recommendation registry functionality."""
from collections import defaultdict
from datetime import datetime
from typing import Dict, List, Optional, Set
import logging
from omegaconf import OmegaConf, DictConfig

from .types import MLRStatus, Recommendation, Source, Evidence, create_config_from_dict
from .identifiers import MLRIdentifierRegistry

logger = logging.getLogger(__name__)

def generate_topic_id(topic: str, recommendation: str) -> str:
    """Generate a unique topic-based ID for a recommendation."""
    import re
    words = recommendation.lower().split()[:5]
    slug = '-'.join(words)
    slug = re.sub(r'[^a-z0-9-]', '', slug)
    return f"{topic.lower().replace(' ', '-')}/{slug}"

class RecommendationRegistry:
    """Registry for ML training recommendations."""
    
    def __init__(self, id_registry: Optional[MLRIdentifierRegistry] = None):
        """Initialize the recommendation registry."""
        self.recommendations: Dict[str, Recommendation] = {}
        self.topic_to_recommendations: Dict[str, List[str]] = defaultdict(list)
        self.id_registry = id_registry or MLRIdentifierRegistry()
        self._config = create_config_from_dict({
            'recommendations': {},
            'metadata': {
                'schema_version': '1.0',
                'last_updated': datetime.now().strftime('%Y-%m-%d')
            }
        })
        logger.info("Initialized recommendation registry")

    def add_recommendation(self, 
                         topic: str,
                         recommendation: str,
                         first_author: str,
                         source_paper: str,
                         year: int,
                         arxiv_id: Optional[str] = None,
                         experimental: bool = False,
                         superseded_by: Optional[str] = None,
                         implementations: Optional[List[str]] = None) -> str:
        """Add a recommendation to the registry."""
        topic_id = generate_topic_id(topic, recommendation)
        paper_id = self.id_registry.get_paper_id(first_author, year, arxiv_id)
        mlr_id = self.id_registry.generate_id(year, paper_id)
        
        source = Source(
            paper=source_paper,
            paper_id=paper_id,
            year=year,
            first_author=first_author,
            arxiv_id=arxiv_id
        )
        
        # Determine status
        if superseded_by:
            status = MLRStatus.DEPRECATED
        elif experimental:
            status = MLRStatus.EXPERIMENTAL
        else:
            status = MLRStatus.STANDARD
            
        rec = Recommendation.create(
            id=mlr_id,
            recommendation=recommendation,
            topic=topic,
            topic_id=topic_id,
            source=source,
            status=status,
            superseded_by=superseded_by,
            deprecated_date=datetime.now().strftime('%Y-%m-%d') if superseded_by else None,
            implementations=implementations or []
        )
        
        self.recommendations[mlr_id] = rec
        self.topic_to_recommendations[topic].append(mlr_id)
        
        logger.info(f"Added recommendation {mlr_id} with status {status}")
        return mlr_id

    def get_recommendation_by_mlr(self, mlr_id: str) -> Optional[Recommendation]:
        """Get a recommendation by its MLR ID."""
        return self.recommendations.get(mlr_id)
    
    def get_recommendations_by_status(self, status: MLRStatus) -> List[Recommendation]:
        """Get all recommendations with a given status."""
        return [rec for rec in self.recommendations.values() if rec.status == status]
    
    def get_recommendations_by_topic(self, topic: str, status: Optional[MLRStatus] = None) -> List[Recommendation]:
        """Get recommendations for a topic, optionally filtered by status."""
        recs = [self.recommendations[mlr_id] for mlr_id in self.topic_to_recommendations[topic]]
        if status:
            recs = [rec for rec in recs if rec.status == status]
        return sorted(recs, key=lambda x: x.source.year)

    def get_topics(self) -> Set[str]:
        """Get all unique topics in the registry."""
        return set(self.topic_to_recommendations.keys())

    def export_registry(self) -> Dict:
        """Export the registry as a list of atomic recommendations."""
        return {
            'metadata': {
                'last_updated': datetime.now().strftime('%Y-%m-%d'),
                'schema_version': '1.0',
                'status_types': [status.value for status in MLRStatus]
            },
            'recommendations': [
                rec.to_dict() for rec in self.recommendations.values()
            ],
            # Include topic stats for informational purposes
            'topics': {
                topic: {
                    'count': len(recs),
                    'years': {
                        'earliest': min(self.recommendations[rid].source.year for rid in recs),
                        'latest': max(self.recommendations[rid].source.year for rid in recs)
                    }
                }
                for topic, recs in self.topic_to_recommendations.items()
            }
        }

def build_registry_from_yaml(yaml_data: Dict) -> RecommendationRegistry:
    """Build a recommendation registry from YAML research data."""
    registry = RecommendationRegistry()
    config = create_config_from_dict(yaml_data)
    
    # Process each year's papers
    for year, papers in config.items():
        for paper in papers:
            # Extract basic paper info
            first_author = paper.first_author
            arxiv_id = paper.get('arxiv_id', None)
            paper_id = f"{first_author} et al. ({year})"
            
            # Process SOTA recommendations
            if hasattr(paper, 'sota') and paper.sota:
                for rec in paper.sota:
                    main_topic = paper.topics[0] if paper.topics else 'general'
                    mlr_id = registry.add_recommendation(
                        topic=main_topic,
                        recommendation=rec,
                        first_author=first_author,
                        source_paper=paper_id,
                        year=int(year),
                        arxiv_id=arxiv_id,
                        implementations=paper.get('models', [])
                    )
                    logger.info(f"Added recommendation {mlr_id}: {rec}")
            
            # Process experimental recommendations
            if paper.get('experimental', False):
                for rec in paper.get('sota', []):
                    mlr_id = registry.add_recommendation(
                        topic=paper.topics[0],
                        recommendation=rec,
                        first_author=first_author,
                        source_paper=paper_id,
                        year=int(year),
                        arxiv_id=arxiv_id,
                        experimental=True
                    )
                    logger.info(f"Added experimental recommendation {mlr_id}: {rec}")
            
            # Process deprecated/superseded recommendations
            if 'attic' in paper and 'superseded_by' in paper.attic:
                for rec in paper.get('sota', []):
                    mlr_id = registry.add_recommendation(
                        topic=paper.topics[0],
                        recommendation=rec,
                        first_author=first_author,
                        source_paper=paper_id,
                        year=int(year),
                        arxiv_id=arxiv_id,
                        superseded_by=paper.attic.superseded_by
                    )
                    logger.info(f"Added deprecated recommendation {mlr_id}: {rec}")

    return registry



================================================================================
File: src/scripts/registry/types.py
================================================================================
# scripts/registry/types.py
"""Type definitions for ML recommendation registry."""

from enum import Enum
from typing import Dict, List, Optional, Union
from dataclasses import dataclass, field
from datetime import datetime
from omegaconf import OmegaConf, DictConfig, MISSING

class MLRStatus(str, Enum):
    """Status states for ML recommendations."""
    EXPERIMENTAL = "experimental"
    STANDARD = "standard"
    DEPRECATED = "deprecated"

@dataclass
class Source:
    """Source information for a recommendation."""
    paper: str = MISSING
    paper_id: str = MISSING
    year: int = MISSING
    first_author: str = MISSING
    arxiv_id: Optional[str] = None

    @classmethod
    def from_dict(cls, data: Union[Dict, DictConfig]) -> 'Source':
        """Create Source from dictionary or DictConfig."""
        conf = OmegaConf.create(data)
        return cls(
            paper=conf.paper,
            paper_id=conf.paper_id,
            year=conf.year,
            first_author=conf.first_author,
            arxiv_id=conf.get('arxiv_id', None)
        )

    def to_dict(self) -> Dict:
        """Convert to dictionary, omitting None values."""
        return {k: v for k, v in OmegaConf.to_container(OmegaConf.create(self)).items() if v is not None}

@dataclass
class Evidence:
    """Supporting evidence for a recommendation."""
    paper: str = MISSING
    paper_id: str = MISSING
    year: int = MISSING
    arxiv_id: Optional[str] = None

    @classmethod
    def from_dict(cls, data: Union[Dict, DictConfig]) -> 'Evidence':
        """Create Evidence from dictionary or DictConfig."""
        conf = OmegaConf.create(data)
        return cls(
            paper=conf.paper,
            paper_id=conf.paper_id,
            year=conf.year,
            arxiv_id=conf.get('arxiv_id', None)
        )

    def to_dict(self) -> Dict:
        """Convert to dictionary, omitting None values."""
        return {k: v for k, v in OmegaConf.to_container(OmegaConf.create(self)).items() if v is not None}

@dataclass
class Recommendation:
    """ML training recommendation."""
    id: str = MISSING
    recommendation: str = MISSING
    topic: str = MISSING
    topic_id: str = MISSING
    source: Source = MISSING
    status: MLRStatus = MISSING
    supporting_evidence: List[Evidence] = field(default_factory=list)
    superseded_by: Optional[str] = None
    deprecated_date: Optional[str] = None
    implementations: List[str] = field(default_factory=list)

    @classmethod
    def create(cls, 
               id: str,
               recommendation: str,
               topic: str,
               topic_id: str,
               source: Union[Dict, DictConfig, Source],
               status: MLRStatus,
               **kwargs) -> 'Recommendation':
        """Create a recommendation from raw data."""
        if isinstance(source, (Dict, DictConfig)):
            source = Source.from_dict(source)
        
        evidence_list = kwargs.get('supporting_evidence', [])
        if evidence_list:
            evidence_list = [
                Evidence.from_dict(e) if isinstance(e, (Dict, DictConfig)) else e
                for e in evidence_list
            ]
        
        return cls(
            id=id,
            recommendation=recommendation,
            topic=topic,
            topic_id=topic_id,
            source=source,
            status=status,
            supporting_evidence=evidence_list,
            superseded_by=kwargs.get('superseded_by'),
            deprecated_date=kwargs.get('deprecated_date'),
            implementations=kwargs.get('implementations', [])
        )

    def to_dict(self) -> Dict:
        """Convert recommendation to dictionary."""
        conf = OmegaConf.create({
            'id': self.id,
            'recommendation': self.recommendation,
            'topic': self.topic,
            'topic_id': self.topic_id,
            'source': self.source.to_dict(),
            'status': self.status.value,
            'supporting_evidence': [e.to_dict() for e in self.supporting_evidence],
            'implementations': self.implementations,
            'superseded_by': self.superseded_by,
            'deprecated_date': self.deprecated_date
        })
        return {k: v for k, v in OmegaConf.to_container(conf).items() if v is not None}

def create_config_from_dict(data: Dict) -> DictConfig:
    """Create an OmegaConf config from a dictionary."""
    return OmegaConf.create(data)



================================================================================
File: src/scripts/utils.py
================================================================================
from pathlib import Path
import tomli
import os
import subprocess
from loguru import logger


def get_project_root() -> Path:
    """
    Get the project root directory by looking for pyproject.toml
    Returns the absolute path to the project root
    """
    current = Path.cwd().absolute()
    
    # Look for pyproject.toml in current and parent directories
    while current != current.parent:
        if (current / 'pyproject.toml').exists():
            return current
        current = current.parent
    
    # If we couldn't find it, use the current working directory
    # and log a warning
    logger.warning("Could not find pyproject.toml in parent directories")
    return Path.cwd().absolute()

def load_config(config_path: str) -> dict:
    """
    Load configuration from a TOML file
    
    Args:
        config_path (str): Path to the TOML configuration file relative to project root
        
    Returns:
        dict: Parsed configuration data
    """
    try:
        full_path = get_project_root() / config_path
        logger.debug(f"Attempting to load config from: {full_path}")
        with open(full_path, "rb") as f:
            return tomli.load(f)
    except FileNotFoundError:
        logger.error(f"Configuration file not found: {full_path}")
        raise

def commit_and_push(
    paths: str | Path | list[str | Path],
    message: str|None = None,
    branch: str|None = None,
    base_branch: str|None = None,
    force: bool = False
) -> None:
    """Commit changes and push to specified or current branch.
    
    Args:
        paths: Path or list of paths to commit
        message: Commit message (defaults to "Update files via automated commit")
        branch: Branch to push to (defaults to current branch)
        base_branch: Optional base branch to create new branch from
        force: If True, create fresh branch and force push (for generated content)
    """
    # Ensure paths is a list and convert to strings
    if isinstance(paths, (str, Path)):
        paths = [paths]
    path_strs = [str(p) for p in paths]
    
    # Set default commit message
    if message is None:
        message = "Update files via automated commit"
    
    # Set up git config
    subprocess.run(["git", "config", "--local", "user.email", "github-actions[bot]@users.noreply.github.com"])
    subprocess.run(["git", "config", "--local", "user.name", "github-actions[bot]"])
    
    # Get current branch if none specified
    if branch is None:
        result = subprocess.run(
            ["git", "rev-parse", "--abbrev-ref", "HEAD"],
            capture_output=True,
            text=True
        )
        branch = result.stdout.strip()
        logger.info(f"Using current branch: {branch}")
    
    if force:
        # Create fresh branch from base_branch or HEAD
        base = base_branch or "HEAD"
        logger.info(f"Creating fresh branch {branch} from {base}")
        subprocess.run(["git", "checkout", "-B", branch, base])
    else:
        # Normal branch handling
        if base_branch:
            logger.info(f"Creating new branch {branch} from {base_branch}")
            subprocess.run(["git", "checkout", "-b", branch, base_branch])
        elif branch != subprocess.run(
            ["git", "rev-parse", "--abbrev-ref", "HEAD"],
            capture_output=True,
            text=True
        ).stdout.strip():
            logger.info(f"Switching to branch {branch}")
            # Try to check out existing branch, create new one if it doesn't exist
            if subprocess.run(["git", "checkout", branch], capture_output=True).returncode != 0:
                subprocess.run(["git", "checkout", "-b", branch])
            subprocess.run(["git", "pull", "origin", branch], capture_output=True)
    
    # Stage and commit changes
    subprocess.run(["git", "add", *path_strs])
    
    # Only commit if there are changes
    result = subprocess.run(
        ["git", "diff", "--staged", "--quiet"],
        capture_output=True
    )
    if result.returncode == 1:  # Changes exist
        logger.info("Committing changes")
        subprocess.run(["git", "commit", "-m", message])
        
        # Push changes
        if force:
            logger.info(f"Force pushing to {branch} branch")
            subprocess.run(["git", "push", "-f", "origin", branch])
        else:
            logger.info(f"Pushing changes to {branch} branch")
            subprocess.run(["git", "push", "origin", branch])
    else:
        logger.info("No changes to commit")


